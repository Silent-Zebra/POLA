Number of agents: 2
Contribution factor: 1.6
Scaled contribution factor? False
Eta: 20
Asymmetric DiCE Updates
Using regular DiCE formulation
Epoch: 0
Eta: 20
Batch size: 64
Inner Steps: [2, 2]
lr_policies: tensor([0.0500, 0.0500])
lr_values: tensor([0.0250, 0.0250])
Policy 0
(Probabilities are for cooperation/contribution, for states 00...0 (no contrib,..., no contrib), 00...01 (only last player contrib), 00...010, 00...011, increasing in binary order ..., 11...11 , start)
tensor([0.4919, 0.5023, 0.5348, 0.4713, 0.5169], grad_fn=<SigmoidBackward>)
Discounted Sum Rewards (Avg over batches) in this episode (removing negative adjustment): 
tensor([ 4.5160, 13.2311])
Max Avg Coop Payout (Truncated Horizon): 13.052
Max Avg Coop Payout (Infinite Horizon): 15.000
Policy 1
(Probabilities are for cooperation/contribution, for states 00...0 (no contrib,..., no contrib), 00...01 (only last player contrib), 00...010, 00...011, increasing in binary order ..., 11...11 , start)
tensor([0.5047, 0.4660, 0.4425, 0.4367, 0.4530], grad_fn=<SigmoidBackward>)
Values 0
tensor([-1.2682e+00,  7.0457e-04, -1.1840e+00, -1.1022e-01, -1.7244e-01],
       requires_grad=True)
Values 1
tensor([0.1287, 0.3474, 7.4395, 6.7568, 0.5439], requires_grad=True)
Epoch: 200
Eta: 20
Batch size: 64
Inner Steps: [2, 2]
lr_policies: tensor([0.0500, 0.0500])
lr_values: tensor([0.0250, 0.0250])
Policy 0
(Probabilities are for cooperation/contribution, for states 00...0 (no contrib,..., no contrib), 00...01 (only last player contrib), 00...010, 00...011, increasing in binary order ..., 11...11 , start)
tensor([0.0362, 0.4660, 0.1975, 0.4472, 0.4111], grad_fn=<SigmoidBackward>)
Discounted Sum Rewards (Avg over batches) in this episode (removing negative adjustment): 
tensor([ 0.9697, -0.2424])
Max Avg Coop Payout (Truncated Horizon): 13.052
Max Avg Coop Payout (Infinite Horizon): 15.000
Policy 1
(Probabilities are for cooperation/contribution, for states 00...0 (no contrib,..., no contrib), 00...01 (only last player contrib), 00...010, 00...011, increasing in binary order ..., 11...11 , start)
tensor([0.0363, 0.1837, 0.4425, 0.4367, 0.3452], grad_fn=<SigmoidBackward>)
Values 0
tensor([-0.1923, -0.0161, -0.2218, -0.1201, -0.3307], requires_grad=True)
Values 1
tensor([-0.2329, -0.2137,  7.4395,  6.7568, -0.3265], requires_grad=True)
Epoch: 400
Eta: 20
Batch size: 64
Inner Steps: [2, 2]
lr_policies: tensor([0.0500, 0.0500])
lr_values: tensor([0.0250, 0.0250])
Policy 0
(Probabilities are for cooperation/contribution, for states 00...0 (no contrib,..., no contrib), 00...01 (only last player contrib), 00...010, 00...011, increasing in binary order ..., 11...11 , start)
tensor([0.0138, 0.4251, 0.1671, 0.4606, 0.3515], grad_fn=<SigmoidBackward>)
Discounted Sum Rewards (Avg over batches) in this episode (removing negative adjustment): 
tensor([ 0.4171, -0.0799])
Max Avg Coop Payout (Truncated Horizon): 13.052
Max Avg Coop Payout (Infinite Horizon): 15.000
Policy 1
(Probabilities are for cooperation/contribution, for states 00...0 (no contrib,..., no contrib), 00...01 (only last player contrib), 00...010, 00...011, increasing in binary order ..., 11...11 , start)
tensor([0.0141, 0.1527, 0.4103, 0.4309, 0.2666], grad_fn=<SigmoidBackward>)
Values 0
tensor([ 0.0616, -0.0595,  0.1988,  0.0189,  0.1323], requires_grad=True)
Values 1
tensor([-0.0618, -0.0512,  7.2641,  6.7263, -0.1444], requires_grad=True)
Epoch: 600
Eta: 20
Batch size: 64
Inner Steps: [2, 2]
lr_policies: tensor([0.0500, 0.0500])
lr_values: tensor([0.0250, 0.0250])
Policy 0
(Probabilities are for cooperation/contribution, for states 00...0 (no contrib,..., no contrib), 00...01 (only last player contrib), 00...010, 00...011, increasing in binary order ..., 11...11 , start)
tensor([0.0022, 0.0796, 0.0743, 0.9778, 0.6510], grad_fn=<SigmoidBackward>)
Discounted Sum Rewards (Avg over batches) in this episode (removing negative adjustment): 
tensor([ 5.8975, 11.7170])
Max Avg Coop Payout (Truncated Horizon): 13.052
Max Avg Coop Payout (Infinite Horizon): 15.000
Policy 1
(Probabilities are for cooperation/contribution, for states 00...0 (no contrib,..., no contrib), 00...01 (only last player contrib), 00...010, 00...011, increasing in binary order ..., 11...11 , start)
tensor([0.0038, 0.0996, 0.7138, 0.7571, 0.1410], grad_fn=<SigmoidBackward>)
Values 0
tensor([12.6223, 17.3039, 16.9795, 16.3251, 15.5714], requires_grad=True)
Values 1
tensor([13.6929, 15.3272, 14.9256, 14.8076, 14.9853], requires_grad=True)
